<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>LLM Anywhere</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Ollama-style chat UI with pluggable backends: On-device WebLLM, Hugging Face, or Google AI Studio. Safe for GitHub Pages via proxy." />
  <style>
    :root{
      --bg:#0b0f14; --panel:#121821; --muted:#6b7280; --text:#e5e7eb; --accent:#93c5fd; --accent2:#34d399; --warn:#fbbf24; --error:#f87171; --border:#1f2937;
    }
    *{box-sizing:border-box}
    body{margin:0;background:var(--bg);color:var(--text);font:16px/1.45 system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial}
    header{position:sticky;top:0;z-index:10;background:rgba(11,15,20,.9);backdrop-filter:saturate(140%) blur(8px);border-bottom:1px solid var(--border)}
    .wrap{max-width:1100px;margin:0 auto;padding:16px}
    .row{display:flex;gap:12px;flex-wrap:wrap;align-items:flex-end}
    .card{background:var(--panel);border:1px solid var(--border);border-radius:12px;padding:12px}
    select,input[type="text"],input[type="number"],textarea{
      width:100%;background:#0f1520;color:var(--text);border:1px solid var(--border);border-radius:10px;padding:10px;outline:none
    }
    label{font-size:.9rem;color:var(--muted);display:block;margin-bottom:6px}
    button{border:1px solid var(--border);background:#0f1520;color:var(--text);padding:10px 14px;border-radius:10px;cursor:pointer}
    button.primary{background:var(--accent);color:#0a0a0a;border-color:transparent}
    button.ghost{background:transparent}
    .chat{display:flex;gap:12px}
    .sidebar{flex:0 0 300px}
    .main{flex:1 1 auto;min-width:0}
    .messages{height:60vh;overflow:auto;background:#0a0f16;border:1px solid var(--border);border-radius:12px;padding:12px}
    .msg{padding:10px 12px;border-radius:10px;margin:10px 0;white-space:pre-wrap;word-wrap:break-word}
    .msg.user{background:#0f1a28}
    .msg.assistant{background:#0f2018}
    .sys{font-size:.85rem;color:var(--muted);margin-top:-4px}
    .footer{display:flex;gap:10px;margin-top:10px}
    .pill{display:inline-block;padding:4px 8px;border:1px solid var(--border);border-radius:999px;font-size:.8rem;color:var(--muted)}
    .kpis{display:flex;gap:8px;flex-wrap:wrap;margin-top:8px}
    .kpi{background:#0e1420;border:1px solid var(--border);border-radius:10px;padding:8px 10px;font-size:.85rem;color:var(--muted)}
    .notice{font-size:.9rem;color:var(--warn)}
    .ok{color:var(--accent2)}
    .err{color:var(--error)}
    @media (max-width: 900px){
      .chat{flex-direction:column}
      .sidebar{flex:1 1 auto}
    }
  </style>

  <!-- Resilient WebLLM loader with multi CDN and ESM fallback -->
  <script>
    async function loadScript(src){
      return new Promise((resolve, reject)=>{
        const s = document.createElement('script');
        s.src = src;
        s.async = true;
        s.onload = ()=>resolve(src);
        s.onerror = ()=>reject(new Error('Failed ' + src));
        document.head.appendChild(s);
      });
    }

    async function tryDynamicImport(url){
      try{
        const mod = await import(url);
        if (mod && (mod.webllm || mod.CreateMLCEngine)){
          // Normalize to window.webllm for the rest of the app
          window.webllm = window.webllm || {};
          if (mod.webllm) Object.assign(window.webllm, mod.webllm);
          if (mod.CreateMLCEngine) window.webllm.CreateMLCEngine = mod.CreateMLCEngine;
          return true;
        }
      }catch(e){}
      return false;
    }

    async function loadWebLLMLibrary(){
      if (window.webllm && window.webllm.CreateMLCEngine) return;

      // Known UMD bundle names across versions
      const UMD = [
        "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.48/dist/index.min.js",
        "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.48/dist/web-llm.min.js",
        "https://unpkg.com/@mlc-ai/web-llm@0.2.48/dist/index.min.js",
        "https://unpkg.com/@mlc-ai/web-llm@0.2.48/dist/web-llm.min.js",
        // Older stable fallbacks
        "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.46/dist/index.min.js",
        "https://unpkg.com/@mlc-ai/web-llm@0.2.46/dist/index.min.js"
      ];

      // Try UMD first
      for (const url of UMD){
        try{
          await loadScript(url);
          if (window.webllm && window.webllm.CreateMLCEngine) return;
        }catch(e){}
      }

      // ESM fallbacks that we convert into window.webllm
      const ESM = [
        "https://esm.run/@mlc-ai/web-llm@0.2.48",
        "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.48/+esm",
        "https://esm.run/@mlc-ai/web-llm@0.2.46",
        "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.46/+esm"
      ];
      for (const url of ESM){
        const ok = await tryDynamicImport(url);
        if (ok && window.webllm && window.webllm.CreateMLCEngine) return;
      }

      throw new Error("WebLLM library failed to load from all sources");
    }
  </script>
</head>
<body>
<header>
  <div class="wrap row">
    <div style="flex:1">
      <div style="font-weight:700">LLM Anywhere</div>
      <div class="sys">Ollama-style chat that runs on GitHub Pages using on-device WebGPU or a secure proxy for cloud APIs.</div>
    </div>
    <div>
      <button id="btn-export" class="ghost" title="Export chat to JSON">Export</button>
      <button id="btn-clear" class="ghost" title="Clear chat">Clear</button>
      <button id="btn-theme" class="ghost" title="Toggle theme">Theme</button>
      <a href="README.html" class="pill" target="_blank" rel="noopener">Docs</a>
    </div>
  </div>
</header>

<div class="wrap chat">
  <aside class="sidebar card">
    <div style="display:grid;gap:12px">
      <div>
        <label>Backend</label>
        <select id="backend">
          <option value="webllm">WebLLM - on device</option>
          <option value="hf">Hugging Face via proxy</option>
          <option value="google">Google AI Studio via proxy</option>
        </select>
      </div>

      <div id="webllm-block">
        <label>WebLLM model</label>
        <select id="webllm-model">
          <option value="Llama-3.2-1B-Instruct-q4f16_1">Llama 3.2 1B Instruct q4f16_1</option>
          <option value="Llama-3.2-1B-Instruct-q4f32_1-MLC">Llama-3.2-1B-Instruct-q4f32_1-MLC</option>
          <option value="Phi-3-mini-4k-instruct-q4f16_1">Phi 3 mini 4k instruct q4f16_1</option>
          <option value="Qwen2.5-1.5B-Instruct-q4f16_1">Qwen2.5 1.5B Instruct q4f16_1</option>
        </select>
        <div class="sys" id="webllm-status">Status: idle</div>
      </div>

      <div id="hf-block" style="display:none">
        <label>HF model id</label>
        <input id="hf-model" type="text" placeholder="meta-llama/Llama-3.1-8B-Instruct" />
        <label>Proxy URL</label>
        <input id="proxy-url-hf" type="text" placeholder="https://your-worker.yourname.workers.dev/hf" />
      </div>

      <div id="google-block" style="display:none">
        <label>Gemini model</label>
        <input id="google-model" type="text" value="gemini-1.5-pro" />
        <label>Proxy URL</label>
        <input id="proxy-url-google" type="text" placeholder="https://your-worker.yourname.workers.dev/google" />
      </div>

      <div>
        <label>System prompt</label>
        <textarea id="system" rows="5" placeholder="You are a helpful, analytical assistant focused on security risk and data integrity."></textarea>
      </div>

      <div class="row">
        <div style="flex:1">
          <label>Temperature</label>
          <input id="temp" type="number" min="0" max="2" step="0.1" value="0.3" />
        </div>
        <div style="flex:1">
          <label>Max tokens</label>
          <input id="max-tokens" type="number" min="64" max="4096" step="64" value="1024" />
        </div>
      </div>

      <div class="kpis">
        <div class="kpi">GPU: <span id="gpu-flag">detecting</span></div>
        <div class="kpi">Msgs: <span id="kpi-msgs">0</span></div>
        <div class="kpi">Backend: <span id="kpi-backend">webllm</span></div>
      </div>

      <div class="notice">For HF or Google, route through the proxy Worker so your API keys stay off the client.</div>
    </div>
  </aside>

  <main class="main">
    <div class="card">
      <div id="messages" class="messages"></div>
      <div class="footer">
        <input id="prompt" type="text" placeholder="Type your message..." />
        <button id="send" class="primary">Send</button>
      </div>
    </div>
  </main>
</div>

<script>
  // Element refs
  const els = {
    backend: document.getElementById('backend'),
    webllmBlock: document.getElementById('webllm-block'),
    hfBlock: document.getElementById('hf-block'),
    googleBlock: document.getElementById('google-block'),
    webllmModel: document.getElementById('webllm-model'),
    hfModel: document.getElementById('hf-model'),
    googleModel: document.getElementById('google-model'),
    proxyHF: document.getElementById('proxy-url-hf'),
    proxyGoogle: document.getElementById('proxy-url-google'),
    system: document.getElementById('system'),
    temp: document.getElementById('temp'),
    maxTokens: document.getElementById('max-tokens'),
    status: document.getElementById('webllm-status'),
    gpu: document.getElementById('gpu-flag'),
    msgs: document.getElementById('messages'),
    send: document.getElementById('send'),
    prompt: document.getElementById('prompt'),
    btnTheme: document.getElementById('btn-theme'),
    btnClear: document.getElementById('btn-clear'),
    btnExport: document.getElementById('btn-export'),
    kpiMsgs: document.getElementById('kpi-msgs'),
    kpiBackend: document.getElementById('kpi-backend'),
  };

  // Theme toggle
  let dark = true;
  els.btnTheme.onclick = () => {
    dark = !dark;
    document.documentElement.style.filter = dark ? '' : 'invert(1) hue-rotate(180deg)';
  };

  // GPU detection with helpful hint
  (async () => {
    try {
      const adapter = await navigator.gpu?.requestAdapter();
      els.gpu.textContent = adapter ? 'WebGPU' : 'CPU';
      if (!adapter){
        document.querySelector('#webllm-status').textContent =
          "WebGPU not detected. Use Chrome or Edge desktop v121+.";
      }
    } catch { els.gpu.textContent = 'CPU'; }
  })();

  // UI state switching
  function onBackendChange(){
    const v = els.backend.value;
    els.webllmBlock.style.display = v === 'webllm' ? '' : 'none';
    els.hfBlock.style.display     = v === 'hf' ? '' : 'none';
    els.googleBlock.style.display = v === 'google' ? '' : 'none';
    els.kpiBackend.textContent = v;
  }
  els.backend.onchange = onBackendChange;
  onBackendChange();

  // Chat state
  let history = [];
  function appendMessage(role, content){
    const div = document.createElement('div');
    div.className = 'msg ' + (role === 'user' ? 'user' : 'assistant');
    div.textContent = content;
    els.msgs.appendChild(div);
    els.msgs.scrollTop = els.msgs.scrollHeight;
  }
  const push = (role, content) => {
    history.push({role, content});
    appendMessage(role, content);
    els.kpiMsgs.textContent = history.filter(m=>m.role!=='system').length;
    localStorage.setItem('llm-anywhere-history', JSON.stringify(history));
  };

  // Load saved chat
  try {
    const saved = localStorage.getItem('llm-anywhere-history');
    if(saved){
      history = JSON.parse(saved);
      history.forEach(m=>appendMessage(m.role, m.content));
      els.kpiMsgs.textContent = history.filter(m=>m.role!=='system').length;
    }
  } catch {}

  // Clear and export
  els.btnClear.onclick = () => { history = []; els.msgs.innerHTML=''; localStorage.removeItem('llm-anywhere-history'); };
  els.btnExport.onclick = () => {
    const blob = new Blob([JSON.stringify(history, null, 2)], {type:'application/json'});
    const a = document.createElement('a'); a.href = URL.createObjectURL(blob); a.download = 'chat-export.json'; a.click();
  };

  // Prompt templating for single prompt providers
  function toPrompt(system, messages){
    const s = system?.trim() ? `<<SYS>>\n${system.trim()}\n<</SYS>>\n\n` : '';
    const body = messages.map(m => {
      const tag = m.role === 'user' ? 'USER' : 'ASSISTANT';
      return `<|${tag}|>\n${m.content}\n`;
    }).join('\n');
    return s + body + '<|ASSISTANT|>\n';
  }

  // WebLLM engine holder
  let webllmEngine = null;

  async function ensureWebLLM(){
    // Early WebGPU check
    if (!('gpu' in navigator)){
      els.status.textContent = "WebGPU not available in this browser";
      throw new Error("WebGPU not available");
    }

    // Load library if needed with resilient strategy
    if (!window.webllm || !window.webllm.CreateMLCEngine){
      els.status.textContent = "Loading WebLLM library...";
      try{
        await loadWebLLMLibrary();
      }catch(e){
        els.status.textContent = "Failed to load WebLLM library";
        throw e;
      }
    }

    const model = els.webllmModel.value;

    // Reuse engine if already loaded for this model
    if (window.webllm && webllmEngine?.getModelId && webllmEngine.getModelId() === model){
      els.status.textContent = "Ready: " + model;
      return;
    }

    els.status.textContent = "Loading model " + model + " ...";
    webllmEngine = await window.webllm.CreateMLCEngine(model, {
      initProgressCallback: (p) => {
        const prog = Math.round(((p && p.progress) ? p.progress : 0) * 100);
        els.status.textContent = `Loading ${prog}%`;
      }
    });
    els.status.textContent = "Ready: " + model;
  }

  async function chatWebLLM(){
    try{
      await ensureWebLLM();
    }catch(e){
      push('assistant', "WebLLM failed: " + (e?.message || e));
      return;
    }

    const messages = [];
    const sys = els.system.value.trim();
    if(sys) messages.push({role:'system', content: sys});
    for(const m of history) if(m.role !== 'system') messages.push(m);

    const params = {
      messages,
      temperature: Number(els.temp.value) || 0.3,
      max_tokens: Number(els.maxTokens.value) || 1024,
      stream: true
    };

    const placeholder = document.createElement('div');
    placeholder.className = 'msg assistant';
    placeholder.textContent = '';
    els.msgs.appendChild(placeholder);
    els.msgs.scrollTop = els.msgs.scrollHeight;

    let full = '';
    const completion = await webllmEngine.chat.completions.create(params);
    for await (const chunk of completion){
      const delta = chunk?.choices?.[0]?.delta?.content || '';
      if(delta){
        full += delta;
        placeholder.textContent = full;
        els.msgs.scrollTop = els.msgs.scrollHeight;
      }
    }
    history.push({role:'assistant', content: full});
    localStorage.setItem('llm-anywhere-history', JSON.stringify(history));
  }

  async function chatHF(){
    const proxy = els.proxyHF.value.trim();
    const model = els.hfModel.value.trim();
    if(!proxy || !model){ alert('Set HF model and Proxy URL'); return; }

    const sys = els.system.value.trim();
    const msgs = history.map(m => ({role:m.role, content:m.content}));
    const prompt = toPrompt(sys, msgs);

    const res = await fetch(proxy, {
      method:'POST',
      headers:{'Content-Type':'application/json'},
      body:JSON.stringify({
        model,
        inputs: prompt,
        parameters:{
          temperature: Number(els.temp.value) || 0.3,
          max_new_tokens: Number(els.maxTokens.value) || 1024,
          top_p: 0.95
        }
      })
    });
    if(!res.ok){ push('assistant', `[HF error] ${res.status} ${await res.text()}`); return; }
    const data = await res.json();
    const text = data.text ?? (Array.isArray(data) ? data[0]?.generated_text : '') ?? '';
    push('assistant', text || '[no output]');
  }

  async function chatGoogle(){
    const proxy = els.proxyGoogle.value.trim();
    const model = els.googleModel.value.trim();
    if(!proxy || !model){ alert('Set Gemini model and Proxy URL'); return; }

    // Convert to Gemini contents schema
    const contents = [];
    const sys = els.system.value.trim();
    if(sys){ contents.push({role:'user', parts:[{text: `System instruction:\n${sys}`}]}); }
    for(const m of history){
      contents.push({ role: m.role === 'user' ? 'user' : 'model', parts: [{text: m.content}] });
    }

    const res = await fetch(proxy, {
      method:'POST',
      headers:{'Content-Type':'application/json'},
      body:JSON.stringify({
        model,
        contents,
        generationConfig:{
          temperature: Number(els.temp.value) || 0.3,
          maxOutputTokens: Number(els.maxTokens.value) || 1024,
          topP: 0.95
        }
      })
    });
    if(!res.ok){ push('assistant', `[Google error] ${res.status} ${await res.text()}`); return; }
    const data = await res.json();
    const text = data.text || data.candidates?.[0]?.content?.parts?.map(p=>p.text).join('') || '';
    push('assistant', text || '[no output]');
  }

  async function sendMessage(){
    const q = els.prompt.value.trim();
    if(!q) return;
    push('user', q);
    els.prompt.value = '';
    try{
      const b = els.backend.value;
      if(b === 'webllm') await chatWebLLM();
      if(b === 'hf') await chatHF();
      if(b === 'google') await chatGoogle();
    }catch(err){
      push('assistant', 'Error: ' + (err?.message || String(err)));
    }
  }
  els.send.onclick = sendMessage;
  els.prompt.addEventListener('keydown', e => { if(e.key==='Enter' && !e.shiftKey){ e.preventDefault(); sendMessage(); }});
</script>
</body>
</html>


